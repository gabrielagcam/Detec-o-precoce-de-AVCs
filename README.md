# üß† Detec√ß√£o precoce de AVCs 

## üìå Descri√ß√£o:

Este reposit√≥rio cont√©m um projeto de Machine Learning para a detec√ß√£o de AVCs (Acidente Vascular Cerebral) utilizando dados do Kaggle e um modelo baseado em Random Forest.

## üìä Dados:

Os dados utilizados foram obtidos no [Kaggle](https://www.kaggle.com/datasets/fedesoriano/stroke-prediction-dataset) e cont√™m informa√ß√µes como idade, g√™nero, hist√≥rico de hipertens√£o, doen√ßas card√≠acas, n√≠vel de glicose no sangue, entre outras vari√°veis relevantes.

## üìù Metodologia:

- [x] **Pr√©-processamento dos dados:** Tratamento de valores nulos, encoding de vari√°veis categ√≥ricas e balanceamento de classes.

- [x] **Divis√£o dos dados:** Separa√ß√£o entre conjunto de treino e teste.

- **Implementa√ß√£o de modelos:**
  - [x] **Random Forest**
  - [x] **SVM**
  - [x] **KNN**
  - [x] **XGBoost**

- [x] **An√°lise dos modelos**

## üõ†Ô∏è Ferramentas utilizadas:

- **Python** 

- **Pandas & NumPy**

- **Matplotlib & Seaborn**

- **Scikit-learn**

- **TensorFlow**


## ü§ñ Descri√ß√£o dos modelos: 

- O **Random Forest** √© um modelo de aprendizagem de m√°quina, muito utilizado em problemas de classifica√ß√£o, que consiste na cria√ß√£o de √°rvores de decis√£o, em que cada uma recebe diferentes subconjuntos de amostras e vari√°veis, de modo que o resultado final √© obtido atrav√©s da combina√ß√£o dessas √°rvores.

- O **SVM (Support Vector Machine)** tamb√©m √© um modelo de aprendizagem supervisionada, e tem como objetivo encontrar o hiperplano de separa√ß√£o ideal que maximiza a margem entre as classes. Para encontrar o hiperplano √≥timo, ele separa as classes maximizando a margem entre os pontos de diferentes classes mais pr√≥ximos (vetores de suporte), de modo que os novos pontos s√£o classificados com base em qual lado do hiperplano caem.

- O **KNN (K-Nearest Neighbors)** √© um modelo que, quando precisa fazer uma previs√£o, calcula a dist√¢ncia (geralmente euclidiana) entre o novo ponto e todos os pontos do conjunto de treino, seleciona os K vizinhos de menor dist√¢ncia e classifica o novo ponto com base na maioria das classes desses vizinhos (voto majorit√°rio).

- O **XGBOOST (Extreme Gradient Boosting)** √© um modelo baseado em √°rvores de decis√£o, no qual m√∫ltiplas √°rvores s√£o combinadas de forma sequencial. A cada etapa, uma nova √°rvore √© adicionada ao conjunto com o objetivo de corrigir os erros cometidos pelas anteriores. O modelo final √© uma combina√ß√£o ponderada de todas essas √°rvores, resultando em uma predi√ß√£o mais precisa e robusta.

## üîç Resultados:

<table>
  <thead>
    <tr>
      <th rowspan="2">Modelo</th>
      <th rowspan="2">Acur√°cia</th>
      <th colspan="2">Precis√£o</th>
      <th colspan="2">Recall</th>
      <th colspan="2">F1-Score</th>
    </tr>
    <tr>
      <th>Classe 0</th>
      <th>Classe 1</th>
      <th>Classe 0</th>
      <th>Classe 1</th>
      <th>Classe 0</th>
      <th>Classe 1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Random Forest</td>
      <td>0.99</td>
      <td>1.00</td>
      <td>0.98</td>
      <td>0.98</td>
      <td>1.00</td>
      <td>0.99</td>
      <td>0.99</td>
    </tr>
    <tr>
      <td>SVM</td>
      <td>0.98</td>
      <td>1.00</td>
      <td>0.97</td>
      <td>0.97</td>
      <td>1.00</td>
      <td>0.98</td>
      <td>0.98</td>
    </tr>
    <tr>
      <td>KNN</td>
      <td>0.95</td>
      <td>1.00</td>
      <td>0.90</td>
      <td>0.89</td>
      <td>1.00</td>
      <td>0.94</td>
      <td>0.95</td>
    </tr>
    <tr>
      <td>XGBoost</td>
      <td>0.97</td>
      <td>1.00</td>
      <td>0.95</td>
      <td>0.95</td>
      <td>1.00</td>
      <td>0.97</td>
      <td>0.98</td>
    </tr>
  </tbody>
</table>

